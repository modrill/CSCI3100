#!/usr/bin/env python
# coding: utf-8
# demo_reco_mix.py
# ============================================================
# 0. é€šç”¨ import
# ============================================================
import os, random, time, warnings, math
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from mysql.connector import connect
import logging
logging.basicConfig(level=logging.INFO)
# ============================================================
# 1. æ•°æ®åº“é…ç½®
# ============================================================
DB_CONFIG = {
    "host": "localhost",
    "port": 3307,
    "user": "root",
    "password": "",
    "database": "buyzu",
    "charset": "utf8mb4"
}

# ============================================================
# 2. ä¸šåŠ¡å‚æ•°
# ============================================================
VEC_NPY   = "item_vec.npy"
REC_XLSX  = "rec_results.xlsx"

MODEL_NAME = "all-roberta-large-v1"

TEXT_DIM   = 256
PRICE_DIM  = 154
BRAND_DIM  = 102

PRICE_ENCODER = "rbf"
RBF_GAMMA     = 50.0

TARGET_DIM = 512
TOPK       = 8
KNN        = 50

BL_RATIO   = 0.8

# SASRecå‚æ•°
MAXLEN = 50
HIDDEN = 64
N_HEAD = 2
N_BLOCK = 2
BATCH   = 128
EPOCH   = 10
LR      = 1e-3
DEVICE  = "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu"

# ============================================================
# 3. æ•°æ®åº“è¯»å–å‡½æ•°
# ============================================================
def fetch_sql(query):
    conn = connect(**DB_CONFIG)
    df = pd.read_sql(query, conn)
    conn.close()

    # åŽ»æŽ‰å°¾éƒ¨ç©ºæ ¼å¹¶å…¨éƒ¨è½¬å°å†™åˆ—å
    if "productID" in df.columns:
        df["productID"] = (
            df["productID"].astype(str)
                        .str.strip()
                        .astype(int)
        )
    return df

# ============================================================
# 3.1 ä¿å­˜æŽ¨èç»“æžœåˆ°æ•°æ®åº“
# ============================================================
def save_to_db(rec_dict, algo="mix"):
    """
    rec_dict = {userKey: [pid1, pid2, ...]}
    """
    import pymysql, datetime
    conn = pymysql.connect(**DB_CONFIG, autocommit=True)
    with conn.cursor() as cur:
        sql = """
        REPLACE INTO recommend_home(userID, productID, rankNo, algoTag, score, createdAt)
        VALUES (%s,%s,%s,%s,%s,%s)
        """
        now = datetime.datetime.now()
        rows = []
        for uid, pids in rec_dict.items():
            for r, pid in enumerate(pids, 1):
                rows.append((uid, str(pid), r, algo, None, now))
        cur.executemany(sql, rows)
    conn.close()
    print(f"âœ… å·²å†™å…¥ recommend_homeï¼š{len(rows)} æ¡")

# ============================================================
# 4. å•†å“å‘é‡ç”Ÿæˆ (é€‚é…æ•°æ®åº“å­—æ®µ)
# ============================================================
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, normalize

def safe_dim(want, n_samples, n_feats):
    max_allowed = min(max(1, n_samples - 1), n_feats)
    if want > max_allowed:
        print(f"âš ï¸  want_dim={want}>{max_allowed}, ç”¨ {max_allowed}")
        want = max_allowed
    return None if want >= n_feats else want

def build_item_vectors():
    if os.path.exists(VEC_NPY):
        print("âœ… å‘é‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å»ºã€‚")
        return

    print("ðŸ”¸ å¼€å§‹æž„å»ºå•†å“å‘é‡...")
    # ä»Žæ•°æ®åº“è¯»å–å•†å“æ•°æ®
    df = fetch_sql("SELECT * FROM products")
    df.rename(columns={"descri": "Description"}, inplace=True)
    
    n = len(df)
    model = SentenceTransformer(MODEL_NAME, device="cpu")

    # æ–‡æœ¬ç‰¹å¾å¤„ç†
    text = (df["productName"].astype(str) + ". " +
            df["Description"].fillna("").astype(str)).tolist()
    txt_full = model.encode(text,
                            batch_size=32,
                            show_progress_bar=True,
                            normalize_embeddings=True)
    d = safe_dim(TEXT_DIM, n, txt_full.shape[1])
    txt_vec = PCA(n_components=d, random_state=42).fit_transform(txt_full) if d else txt_full
    txt_vec = normalize(txt_vec.astype("float32"))

    # ä»·æ ¼ç‰¹å¾å¤„ç†
    price = MinMaxScaler().fit_transform(df["price"].values.reshape(-1,1))
    centers = np.linspace(0,1,PRICE_DIM,dtype="float32")
    price_vec = np.exp(-RBF_GAMMA * (price - centers)**2)
    price_vec = normalize(price_vec, norm="l2")

    # å“ç‰Œç‰¹å¾å¤„ç†
    brands = fetch_sql("SELECT brandID, brandName FROM brand")
    brand_map = dict(zip(brands["brandID"], brands["brandName"]))
    df["brand_str"] = df["brandID"].map(brand_map).fillna("Unknown")
    
    brand_full = model.encode(df["brand_str"].tolist(),
                              batch_size=32,
                              normalize_embeddings=True)
    d = safe_dim(BRAND_DIM, n, brand_full.shape[1])
    brand_vec = PCA(n_components=d, random_state=42).fit_transform(brand_full) if d else brand_full
    brand_vec = normalize(brand_vec.astype("float32"))

    # åˆå¹¶ç‰¹å¾
    emb = np.hstack([txt_vec, price_vec, brand_vec]).astype("float32")
    d = safe_dim(TARGET_DIM, *emb.shape)
    if d:
        emb = PCA(n_components=d, random_state=42).fit_transform(emb)
    vec = normalize(emb.astype("float32"), norm="l2")
    np.save(VEC_NPY, vec)
    print(f"âœ… å•†å“å‘é‡ä¿å­˜åˆ° {VEC_NPY}")

# ============================================================
# 5. BaselineæŽ¨è (ä¿æŒåŽŸé€»è¾‘)
# ============================================================
from sklearn.neighbors import NearestNeighbors
def get_baseline_candidates(vec, meta, hist, id2idx):
    nbrs = NearestNeighbors(metric="cosine", algorithm="brute")
    nbrs.fit(vec)
    idx2pid = meta["productID"].tolist()
    cands_dict = {}
    for uid, seq in hist.items():
        user_vec = normalize(vec[[id2idx[i] for i in seq]].mean(0,keepdims=True))
        _, ind = nbrs.kneighbors(user_vec, n_neighbors=min(KNN, len(vec)))
        cands = [int(idx2pid[i]) for i in ind[0] if idx2pid[i] not in seq][:TOPK]
        cands_dict[uid] = cands
    return cands_dict

# ============================================================
# 6. SASRecæ¨¡åž‹ (ä¿æŒåŽŸé€»è¾‘)
# ============================================================
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import DataLoader, Dataset

class SeqDataset(Dataset):
    def __init__(self, hist, id2idx):
        self.data = []
        for seq in hist.values():
            idx = [id2idx[i] for i in seq]
            for t in range(1, len(idx)):
                h = idx[max(0, t - MAXLEN):t]
                self.data.append((h, idx[t]))
    def __len__(self): return len(self.data)
    def __getitem__(self, i):
        h, tgt = self.data[i]
        pad = MAXLEN - len(h)
        return (torch.tensor([0] * pad + h),
                torch.tensor(len(h) - 1 if h else 0),
                torch.tensor(tgt))

class SASRec(nn.Module):
    def __init__(self, emb):
        super().__init__()
        n, dim = emb.shape
        self.static = nn.Embedding.from_pretrained(torch.tensor(emb), freeze=True)
        self.proj   = nn.Linear(dim, HIDDEN, bias=False)
        self.pos    = nn.Embedding(MAXLEN, HIDDEN)
        layer = nn.TransformerEncoderLayer(HIDDEN, N_HEAD, 4*HIDDEN,
                                           batch_first=True)
        self.enc  = nn.TransformerEncoder(layer, N_BLOCK)
        self.norm = nn.LayerNorm(HIDDEN)
    def forward(self, seq, seqlen):
        x = self.proj(self.static(seq)) + self.pos(
            torch.arange(seq.size(1), device=seq.device))
        x = self.norm(x)
        x = self.enc(x)
        o = x[torch.arange(x.size(0)), seqlen]
        logits = o @ self.proj(self.static.weight).T
        return logits

def train_sasrec(vec, hist, id2idx):
    ds = SeqDataset(hist, id2idx)
    if len(ds) == 0:
        print("âš ï¸ æ— ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œè·³è¿‡ SASRec è®­ç»ƒ")
        return None
    dl = DataLoader(ds, batch_size=BATCH, shuffle=True)
    m  = SASRec(vec).to(DEVICE)
    opt = optim.Adam(m.parameters(), lr=LR)
    cel = nn.CrossEntropyLoss()
    for ep in range(1, EPOCH+1):
        m.train()
        total_loss = 0
        for s, l, t in dl:
            s, l, t = s.to(DEVICE), l.to(DEVICE), t.to(DEVICE)
            logits = m(s, l)
            loss   = cel(logits, t)
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item() * len(t)
        print(f"Epoch {ep}/{EPOCH}  loss={total_loss/len(ds):.4f}")
    return m

def get_sasrec_candidates(model, vec, meta, hist, id2idx):
    if model is None:
        return {u: [] for u in hist}
    idx2pid = meta["productID"].tolist()
    out = {}
    model.eval()
    for uid, seq in hist.items():
        if not seq:
            out[uid] = []
            continue
        idx = [id2idx[i] for i in seq[-MAXLEN:]]
        pad = MAXLEN - len(idx)
        seq_t = torch.tensor([0] * pad + idx, device=DEVICE).unsqueeze(0)
        logit = model(seq_t, torch.tensor([len(idx)-1], device=DEVICE))
        top = torch.topk(logit, TOPK+len(seq)).indices[0].tolist()
        cand = []
        for j in top:
            pid = idx2pid[j]
            if pid not in seq:
                cand.append(pid)
            if len(cand) == TOPK:
                break
        out[uid] = cand
    return out

# ============================================================
# 7. æ··åˆ & è¾“å‡º
# ============================================================
def mix_and_save(baseline, sasrec, meta, hist, id2idx,
                 topk=TOPK, bl_ratio=BL_RATIO,
                 write_excel=True, write_db=True, algo="mix"):
    idx2pid = meta["productID"].tolist()
    n_bl = int(round(topk * bl_ratio))
    n_sa = topk - n_bl

    hit = tot = 0
    rows = []
    rec_dict = {}

    for uid, seq in hist.items():
        bl = baseline.get(uid, [])
        sa = sasrec.get(uid, [])
        mix = []

        # 1. baseline
        for pid in bl:
            if pid not in mix:
                mix.append(pid)
            if len(mix) == n_bl:
                break

        # 2. sasrec
        for pid in sa:
            if pid not in mix:
                mix.append(pid)
            if len(mix) == topk:
                break

        # 3. ä¸è¶³ topkï¼Œå†è¡¥
        for pid in bl + sa:
            if pid not in mix:
                mix.append(pid)
            if len(mix) == topk:
                break

        # 4. è¯„ä¼°
        if len(seq) >= 2:
            tot += 1
            hit += int(seq[-1] in mix)

        # 5. ç»„ç»‡ä¸€è¡Œå†™ Excel
        row = [uid]
        rec_pid_list = []
        for pid in mix:
            rec_pid_list.append(pid)
            prod = meta[meta["productID"] == pid].iloc[0]
            row.extend([
                pid,
                prod["productName"],
                prod["brand"],
                prod["price"]
            ])
        while len(row) < 1 + topk*4:
            row.extend([""]*4)
        rows.append(row)

        rec_dict[uid] = rec_pid_list

    if write_db:
        save_to_db(rec_dict, algo=algo)
    if write_excel:
        cols = ["userID"] + [
            f"rec{k}_{c}"
            for k in range(1, topk+1)
            for c in ("id","name","brand","price")
        ]
        pd.DataFrame(rows, columns=cols).to_excel(REC_XLSX, index=False)
        print(f"ðŸ“„ ç»Ÿä¸€ç»“æžœä¿å­˜è‡³ {REC_XLSX}")

    if tot:
        print(f"ðŸŽ¯ Mix Hit@{topk}: {hit/tot:.4f}")
    else:
        print("âš ï¸ æ— æ³•è¯„ä¼°")

# ============================================================
# 8. ä¸»æ‰§è¡Œæµç¨‹å°è£…ä¸º main()
# ============================================================
def main():
    
    # ç”Ÿæˆå•†å“å‘é‡
    build_item_vectors()

    # ---------- è¯»å•†å“è¡¨ ----------
    print("ðŸ”¸ åŠ è½½å•†å“å…ƒæ•°æ®...")
    meta = fetch_sql("SELECT * FROM products")

    # ---------- è¯»å“ç‰Œè¡¨å¹¶æ˜ å°„ ----------
    print("ðŸ”¸ åŠ è½½å“ç‰Œè¡¨...")
    brand = fetch_sql("SELECT brandID, brandName FROM brand")
    brand_map = dict(zip(brand["brandID"], brand["brandName"]))
    meta["brand"] = meta["brandID"].map(brand_map).fillna("Unknown")

    # ---------- è¯»è´­ç‰©è½¦æ—¥å¿— ----------
    print("ðŸ”¸ åŠ è½½è´­ç‰©è½¦æ—¥å¿—...")
    df = fetch_sql("""
        SELECT 
            CAST(userID AS CHAR) AS uid,  -- å¦‚æžœå¸Œæœ› uid ä¸ºå­—ç¬¦ä¸²ï¼›ä¹Ÿå¯ç›´æŽ¥ä½¿ç”¨ userID
            productID,
            createdAt
        FROM carts
        ORDER BY createdAt
    """)
    print(df)

    # ---------- æž„å»ºåŽ†å² ----------
    id2idx = {pid: i for i, pid in enumerate(meta["productID"])}
    hist = {}
    for r in df.itertuples(index=False):
        hist.setdefault(r.uid, []).append(int(r.productID))
    print("ðŸªµ hist ç”¨æˆ·æ•° =", len(hist))

    # åŠ è½½å‘é‡
    vec = np.load(VEC_NPY)

    # åŸºçº¿å€™é€‰
    print("ðŸ”¹ ç”Ÿæˆ Baseline å€™é€‰...")
    baseline = get_baseline_candidates(vec, meta, hist, id2idx)

    # SASRec å€™é€‰
    print("ðŸ”¸ è®­ç»ƒ & ç”Ÿæˆ SASRec å€™é€‰...")
    sas_model = train_sasrec(vec, hist, id2idx)
    sasrec    = get_sasrec_candidates(sas_model, vec, meta, hist, id2idx)

    # æ··åˆ & è¾“å‡º
    print("ðŸ”» æ··åˆ & è¾“å‡º...")
    mix_and_save(baseline, sasrec, meta, hist, id2idx)

if __name__ == "__main__":
    main()